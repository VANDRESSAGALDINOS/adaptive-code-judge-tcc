#!/usr/bin/env python3
"""
Script de Checklist para Experimentos - TCC Adaptive Code Judge
Executa ap√≥s cada experimento para garantir coleta completa de dados
"""

import json
import os
from pathlib import Path
from datetime import datetime

class ExperimentChecklist:
    def __init__(self, experiment_path):
        self.path = Path(experiment_path)
        self.categoria = self.path.parent.name
        self.problema = self.path.name
        
    def check_required_files(self):
        """Verifica se todos os arquivos obrigat√≥rios existem"""
        required_files = [
            'final_report.json',
            'results/calibration_*.json',
            'results/validation_results.json', 
            'results/slow_solution_validation.json',
            'resultados_finais/README.md'
        ]
        
        missing = []
        for file_pattern in required_files:
            if '*' in file_pattern:
                if not list(self.path.glob(file_pattern)):
                    missing.append(file_pattern)
            else:
                if not (self.path / file_pattern).exists():
                    missing.append(file_pattern)
        
        return missing
    
    def extract_key_metrics(self):
        """Extrai m√©tricas chave para os gr√°ficos finais"""
        try:
            with open(self.path / 'final_report.json', 'r') as f:
                report = json.load(f)
            
            # Extrair dados estruturados
            metrics = {
                'experiment': {
                    'categoria': self.categoria,
                    'problema': self.problema,
                    'data': datetime.now().isoformat()
                },
                'fatores': {
                    'ajuste_mediano': report.get('calibration_analysis', {}).get('adjustment_factor', 0),
                    'ajuste_iqr': 0,  # Calcular do relat√≥rio
                    'confiabilidade': 'Alta' if report.get('calibration_analysis', {}).get('reliable', False) else 'Baixa'
                },
                'validacao': {
                    'cses_real': 0,  # Extrair dos metadados
                    'nosso_benchmark': 0,  # Calcular
                    'correlacao': 0
                },
                'seletividade': {
                    'preservada': report.get('slow_solution_validation', {}).get('overall_selectivity', {}).get('preserved', False)
                }
            }
            
            return metrics
            
        except Exception as e:
            print(f"‚ùå Erro ao extrair m√©tricas: {e}")
            return None
    
    def generate_checklist(self):
        """Gera checklist interativo"""
        print(f"\nüîç CHECKLIST - {self.categoria.upper()}/{self.problema}")
        print("=" * 60)
        
        # 1. Arquivos obrigat√≥rios
        print("\nüìÅ 1. ARQUIVOS OBRIGAT√ìRIOS:")
        missing = self.check_required_files()
        if not missing:
            print("   ‚úÖ Todos os arquivos necess√°rios est√£o presentes")
        else:
            print(f"   ‚ùå Arquivos faltando: {missing}")
            return False
        
        # 2. M√©tricas chave
        print("\nüìä 2. M√âTRICAS PARA GR√ÅFICOS:")
        metrics = self.extract_key_metrics()
        if metrics:
            print(f"   ‚úÖ Fator de ajuste: {metrics['fatores']['ajuste_mediano']:.2f}x")
            print(f"   ‚úÖ Confiabilidade: {metrics['fatores']['confiabilidade']}")
            print(f"   ‚úÖ Seletividade preservada: {metrics['seletividade']['preservada']}")
        else:
            print("   ‚ùå Erro ao extrair m√©tricas")
            return False
        
        # 3. Checklist manual
        print("\n‚úã 3. VERIFICA√á√ÉO MANUAL:")
        manual_checks = [
            "Fator de ajuste √© realista (< 50x)?",
            "Pelo menos 3 tamanhos de input testados?",
            "Slow solutions deram TLE em ambos sistemas?",
            "Correla√ß√£o com plataforma real verificada?",
            "Insights espec√≠ficos documentados?",
            "Anomalias identificadas e explicadas?"
        ]
        
        all_ok = True
        for check in manual_checks:
            response = input(f"   {check} [y/N]: ").strip().lower()
            if response != 'y':
                all_ok = False
                print(f"   ‚ö†Ô∏è  Pendente: {check}")
        
        # 4. Classifica√ß√£o para gr√°ficos
        print("\nüéØ 4. CLASSIFICA√á√ÉO PARA AN√ÅLISE FINAL:")
        
        emblem√°tico = input("   Este caso √© emblem√°tico da categoria? [y/N]: ").strip().lower() == 'y'
        representa_bem = input("   Representa bem o comportamento t√≠pico? [y/N]: ").strip().lower() == 'y'
        dados_suficientes = input("   Dados suficientes para generaliza√ß√£o? [y/N]: ").strip().lower() == 'y'
        
        if emblem√°tico and representa_bem and dados_suficientes:
            prioridade = "Alta"
        elif representa_bem and dados_suficientes:
            prioridade = "M√©dia"
        else:
            prioridade = "Baixa"
        
        print(f"   üèÜ Prioridade para gr√°ficos finais: {prioridade}")
        
        # 5. Salvar metadados
        metadata = {
            **metrics,
            'qualidade': {
                'completo': all_ok,
                'emblem√°tico': emblem√°tico,
                'representa_categoria': representa_bem,
                'dados_suficientes': dados_suficientes,
                'prioridade_graficos': prioridade
            },
            'timestamp_checklist': datetime.now().isoformat()
        }
        
        output_file = self.path / 'metadata_graficos.json'
        with open(output_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"\nüíæ Metadados salvos em: {output_file}")
        
        return all_ok and emblem√°tico
    
    def generate_summary(self):
        """Gera resumo executivo do experimento"""
        try:
            with open(self.path / 'metadata_graficos.json', 'r') as f:
                meta = json.load(f)
            
            summary = f"""
# RESUMO EXECUTIVO - {self.categoria.upper()}/{self.problema}

## üéØ RESULTADOS PRINCIPAIS
- **Fator de Ajuste**: {meta['fatores']['ajuste_mediano']:.2f}x
- **Confiabilidade**: {meta['fatores']['confiabilidade']}
- **Seletividade**: {'‚úÖ Preservada' if meta['seletividade']['preservada'] else '‚ùå Comprometida'}

## üìä CLASSIFICA√á√ÉO
- **Prioridade Gr√°ficos**: {meta['qualidade']['prioridade_graficos']}
- **Emblem√°tico**: {'‚úÖ' if meta['qualidade']['emblem√°tico'] else '‚ùå'}
- **Representa Categoria**: {'‚úÖ' if meta['qualidade']['representa_categoria'] else '‚ùå'}

## üîó PARA AN√ÅLISE FINAL
- [ ] Incluir no gr√°fico executivo de fatores
- [ ] {'‚úÖ' if meta['qualidade']['emblem√°tico'] else '‚ùå'} Caso narrativo individual
- [ ] Incluir no heatmap de injusti√ßa
- [ ] Incluir na valida√ß√£o externa

---
Gerado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
            
            with open(self.path / 'RESUMO_EXECUTIVO.md', 'w') as f:
                f.write(summary)
            
            print("üìã Resumo executivo gerado!")
            
        except Exception as e:
            print(f"‚ùå Erro ao gerar resumo: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 2:
        print("Uso: python CHECKLIST_EXPERIMENTO.py <caminho_do_experimento>")
        sys.exit(1)
    
    experiment_path = sys.argv[1]
    checker = ExperimentChecklist(experiment_path)
    
    print("üöÄ INICIANDO CHECKLIST P√ìS-EXPERIMENTO")
    
    if checker.generate_checklist():
        checker.generate_summary()
        print("\nüéâ EXPERIMENTO VALIDADO E PRONTO PARA AN√ÅLISE FINAL!")
    else:
        print("\n‚ö†Ô∏è  EXPERIMENTO PRECISA DE AJUSTES ANTES DA AN√ÅLISE FINAL")

